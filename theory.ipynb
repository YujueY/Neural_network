{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "将输入按权重进行求和，然后用激活函数将求和值输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "线性激活函数的网络深度没有意义，所以使用非线性激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic回归模型使用的Loss函数为Logistic Loss函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "A. ReLU\n",
    "B. Leaky ReLU\n",
    "C. sigmoid\n",
    "D. tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C、D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "因为神经网络的对称性，若参数都为0，则每个神经元的值都一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Can you implement the softmax function using python ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0.05943317 0.16155612 0.05943317 0.16155612 0.05943317 0.05943317\n",
      "  0.43915506]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def softmax(inMatrix):\n",
    "    \"\"\"\n",
    "    softmax计算公式函数\n",
    "    :param inMatrix: 矩阵数据\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m,n = np.shape(inMatrix)  #得到m,n(行，列)\n",
    "    outMatrix = np.mat(np.zeros((m,n)))  #mat生成数组\n",
    "    soft_sum = 0\n",
    "    for idx in range(0,n):\n",
    "        outMatrix[0,idx] = math.exp(inMatrix[0,idx])  #求幂运算，取e为底的指数计算变成非负\n",
    "        soft_sum +=outMatrix[0,idx]   #求和运算\n",
    "    for idx in range(0,n):\n",
    "        outMatrix[0,idx] = outMatrix[0,idx] /soft_sum #然后除以所有项之后进行归一化\n",
    "    return outMatrix\n",
    "\n",
    "\n",
    "a = np.array([[1,2,1,2,1,1,3]])\n",
    "print(softmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
